{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tf-idf-vectorization.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xb34NrowDDs9"
      },
      "source": [
        "# TF-IDF - “Term Frequency — Inverse Document Frequency”\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbEo5rtFCY7k"
      },
      "source": [
        "Bu teknik, belgelerdeki bir sözcüğü ölçmek için kullanılan bir tekniktir, genellikle her sözcük için belgede ve corpus'da sözcüğün önemini belirten bir ağırlık değeri hesaplarız. Bu yöntem, Information Retrieval ve Text Mining'de yaygın olarak kullanılan bir tekniktir.\n",
        "\n",
        "TF-IDF = Term Frequency (TF) * Inverse Document Frequency (IDF)\n",
        "\n",
        "_Terminology_\n",
        "* t — term (word)\n",
        "* d — document (set of words)\n",
        "* N — count of corpus\n",
        "* corpus — the total document set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzuTkw9qDT8C"
      },
      "source": [
        "## Term Frequency"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpNXXmxJDVf0"
      },
      "source": [
        "TF, bir document'de bir kelimenin ne kadar sıklıkta bulunduğunu ölçer.\n",
        "\n",
        "TF bu şekilde formülize edilir.\n",
        "\n",
        "tf(t,d) = count of t in d / number of words in d"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "No9wX5Y-Em2-"
      },
      "source": [
        "## Document Frequency"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGsugWdVFAqI"
      },
      "source": [
        "DF, tüm corpus kümesi içerisinde document'in önemini ölçer, bu TF'ye çok benzer. Tek fark, TF'nin, d belgesindeki bir t terimi için frekans sayacı olmasıdır, burada DF, N belge kümesindeki yani corpus'undaki t teriminin geçme sayısıdır. Diğer bir deyişle, DF, t teriminin geçtiği document sayısıdır. \n",
        "t terimi document'te en az bir kez yer alıyorsa, t terimi document'te yer alır deriz, terimin d document'i içinde kaç kez bulunduğunu bilmemize gerek yoktur.\n",
        "\n",
        "df(t) = occurrence of t in documents\n",
        "\n",
        "Bu değerleri de bir aralıkta tutmak için, df'i toplam belge sayısına bölerek normalize etmemiz gerekir. Asıl amacımız bir terimin bilgilendirici bir terim mi yoksa değil mi olduğunu bilmektir ve DF bunun tam tersidir."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDZo-sCfGKYK"
      },
      "source": [
        "## Inverse Document Frequency"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MuypQl6Ltsv"
      },
      "source": [
        "IDF, t teriminin bilgilendiriciliğini ölçen Document Frequency'nin tersidir.\n",
        "IDF'yi hesapladığımızda, stop word'ler gibi en çok ortaya çıkan sözcükler için bu değer çok düşük olacaktır (çünkü \"is\" gibi stop word'ler hemen hemen tüm belgelerde mevcuttur ve N / df bu sözcüğe çok düşük bir değer verecektir. ). Bu istediğimiz şeydir, anlamına göre bir ağırlık verilmiş olacak. \n",
        "\n",
        "idf(t) = N/df\n",
        "\n",
        "Yukarıdaki formül ile ilgili birkaç sorun ile karşılacağız ki onlar şunlardır; \n",
        "örneğin, çok büyük corpus'larda mesela 10.000 document'e sahip corpus'da yukarıdaki denklem çok yüksek değerler verecektir. Bu büyük değerleri normalleştirmek için idf(t) = N/df in logaritmasını alacağız.\n",
        "Bir sorgulama yaptığımızda ve kelime corpus içerisinde yoksa df 0 sıfır olacaktır. Bir sayıyı sıfıra bölmek istemediğimiz için ise paydaya 1 ekleyip değeri biraz daha sabit ve anlamlı bir hale dönüştürüyoruz.\n",
        "\n",
        "**idf(t) = log(N/(df + 1))**\n",
        "\n",
        "Son olarak, TF ve IDF değerlerini çarpım işleminden geçirerek TF-IDF skorunu elde etmiş olacağız. \n",
        "\n",
        "### tf-idf(t, d) = tf(t, d) * log(N/(df + 1)) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GHUI445Gg2C"
      },
      "source": [
        "## Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJOtj9BFGkEb",
        "outputId": "dc666742-6934-4d99-b427-2be9aa5edf07",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install num2words\n",
        "!pip install TurkishStemmer\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "#from TurkishStemmer import TurkishStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from collections import Counter\n",
        "from num2words import num2words\n",
        "\n",
        "import nltk\n",
        "import os\n",
        "import string\n",
        "import numpy as np\n",
        "import copy\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import re\n",
        "import math\n",
        "import sklearn as sk"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting num2words\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/eb/a2/ea800689730732e27711c41beed4b2a129b34974435bdc450377ec407738/num2words-0.5.10-py3-none-any.whl (101kB)\n",
            "\r\u001b[K     |███▎                            | 10kB 19.8MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 20kB 15.0MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 30kB 13.8MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 40kB 13.4MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 51kB 11.2MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 61kB 11.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 71kB 11.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 81kB 12.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 92kB 12.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 102kB 7.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: docopt>=0.6.2 in /usr/local/lib/python3.6/dist-packages (from num2words) (0.6.2)\n",
            "Installing collected packages: num2words\n",
            "Successfully installed num2words-0.5.10\n",
            "Collecting TurkishStemmer\n",
            "  Downloading https://files.pythonhosted.org/packages/fd/bf/3e56dd4ce442f9237e1c202ce736ae5e5818d74f81604f1665e67736cfc0/TurkishStemmer-1.3-py3-none-any.whl\n",
            "Installing collected packages: TurkishStemmer\n",
            "Successfully installed TurkishStemmer-1.3\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4Lc1qTMGvnl"
      },
      "source": [
        "## Documents"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhjLVTOHGxkK"
      },
      "source": [
        "docA = \"\"\"Elon Musk is a South African-born American industrial engineer, entrepreneur, who co-founded Paypal and founded aerospace transportation services company SpaceX. He is also one of the early investors in Tesla, an electric car company, and now the Chief Executive Officer of the firm as well. With a $70.5 bn fortune, Musk sits comfortably at the 7th spot in the world's top 10 billionaires list.\n",
        "Early years\n",
        " \n",
        "Born in 1971, Elon Musk displayed an early talent in computers and video games and at the age of 12 created a video game code that earned him fame, as well as a fortune. He acquired a Canadian passport to avoid supporting apartheid in Africa at the time and also due to greater economic opportunities in the United States.\n",
        " \n",
        "Paypal and SpaceX\n",
        " \n",
        "Musk has created several successful firms since he left Stanford University in 1997. That year, he created the first company - Zip2. The firm,  which provided maps and business directories to online newspapers, was bought by Compaq in 1999. Then he created X.com which eventually merged with PayPal, which went public in 2001, and in 2002 eBay bought the firm for $1.5 bn. Musk in 2002, formed SpaceX and since then the aerospace firm has covered several milestones over the years. SpaceX became the first company to successfully relaunch and land at the first stage of an orbital rocket in late 2017. In 2020, SpaceX made history on May 30, after it flew NASA astronauts Doug Hurley and Bob Behnken, to space aboard its Crew Dragon spacecraft using a Falcon 9 rocket.\n",
        " \n",
        "Tesla\n",
        " \n",
        "In 2004, Musk invested heavily in Tesla, an electric car company as he believed electric vehicles to be the future of mobility. Two years after introducing the first car, Tesla, in 2008, introduced the Model S sedan, which was praised by automotive critics for its performance and design. The company eventually made him a billionaire at the age of 40 in 2012. Musk, as the CEO of Tesla, landed himself in several controversies which resulted in stocks tanking by a huge margin. In 2018, Musk ran into trouble after his tweet falsely claimed that he had secured funding and was considering taking Tesla private at $420 per share. It resulted in both him and Tesla paying a $20 million fine and Musk agreeing to step down as chairman of Tesla's board. However, despite controversies, Tesla stocks have continued to grow adding to his wealth\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "docB = \"\"\"Born in 1971 in South Africa of a model and dietitian, Maye Musk, and an electromechanical engineer, Errol Musk, whom Elon has described as \"a terrible human being,\" Elon Reeve Musk is the eldest of his parents' three children, and a citizen of three countries: South Africa, Canada, and the US.\n",
        "\n",
        "Musk spent his childhood with his nose in books and computers. A small, introverted boy, he was ostracized by his schoolmates and regularly beaten up by class bullies, until he became big enough to defend himself after a growth spurt in his teens.\n",
        "Musk moved to Silicon Valley in summer 1995. He registered in a PhD program in applied physics at Stanford University – but withdrew after only two days. His brother Kimball Musk, who is 15 months younger than Elon, had just graduated from Queen's University with a business degree and come to join him in California.The early Internet was heating up, and the brothers decided to launch a startup they called Zip2, an online business directory equipped with maps.\n",
        "\n",
        "In due course, the brothers found angel investors for Zip2 and built it into a successful company. In 1999, the brothers sold Zip2 to computer maker Compaq for $307 million (280 million).\n",
        "\n",
        "Elon then founded an online financial services company, X.com, on his own. Its main rival was a company called Confinity, founded by Peter Thiel and two others just months after X.com, with offices in the same building. The two companies merged in March 2000 and took on the name of their main product, PayPal, a person-to-person online money transfer service.\n",
        "\n",
        "Ebay, the online auction service, bought PayPal in October 2002 for $1.5 billion worth of Ebay shares. At the age of 31, Elon Musk, who had been the largest shareholder in PayPal with 11.7% of its equity shares, found himself holding $165 million worth of Ebay stock.\"\"\"\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nWd1ZoPG1CT"
      },
      "source": [
        "# Corpus oluşturdum.\n",
        "corpus = []\n",
        "corpus.append(docA)\n",
        "corpus.append(docB)\n",
        "N = len(corpus)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GT02PvNOpJQ"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXKym3o4Ou-b"
      },
      "source": [
        "Bu noktada bazı yapılması zorunlu diyebileceğimiz adımları izleyeceğim. Bu işlemler için gerekli fonksiyonları tanımlayacağım.\n",
        "Adımlar şu şekilde; \n",
        "* corpus içerisindeki tüm kelimeleri lowercase yapmak, \n",
        "* noktalama işaretlerini silmek, \n",
        "* stop word'leri silmek, \n",
        "* Stemming."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8SCFzRknFxFD"
      },
      "source": [
        "def convert_lower_case(data):\n",
        "    return np.char.lower(data)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4PjDQOVFxHt"
      },
      "source": [
        "def remove_stop_words(data):\n",
        "    stop_words = stopwords.words('english')\n",
        "    words = word_tokenize(str(data))\n",
        "    new_text = \"\"\n",
        "    for w in words:\n",
        "        if w not in stop_words and len(w) > 1:\n",
        "            new_text = new_text + \" \" + w\n",
        "    return new_text"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5a9dh9pFw1k"
      },
      "source": [
        "def remove_punctuation(data):\n",
        "    symbols = \"!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\\n\"\n",
        "    for i in range(len(symbols)):\n",
        "        data = np.char.replace(data, symbols[i], ' ')\n",
        "        data = np.char.replace(data, \"  \", \" \")\n",
        "    data = np.char.replace(data, ',', '')\n",
        "    return data"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAtkCMWfCS3q"
      },
      "source": [
        "def remove_apostrophe(data):\n",
        "    return np.char.replace(data, \"'\", \"\")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVlDOo6SHK2M"
      },
      "source": [
        "def stemming(data):\n",
        "    stemmer = PorterStemmer()\n",
        "    \n",
        "    tokens = word_tokenize(str(data))\n",
        "    new_text = \"\"\n",
        "    for w in tokens:\n",
        "        new_text = new_text + \" \" + stemmer.stem(w)\n",
        "    return new_text"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGYjHEfnHLSZ"
      },
      "source": [
        "def convert_numbers(data):\n",
        "    tokens = word_tokenize(str(data))\n",
        "    new_text = \"\"\n",
        "    for w in tokens:\n",
        "        try:\n",
        "            w = num2words(int(w), lang='eng')\n",
        "        except:\n",
        "            a = 0\n",
        "        new_text = new_text + \" \" + w\n",
        "    new_text = np.char.replace(new_text, \"-\", \" \")\n",
        "    return new_text"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjSMfA-dHMsB"
      },
      "source": [
        "def preprocess(data):\n",
        "    data = convert_lower_case(data)\n",
        "    data = remove_punctuation(data) \n",
        "    data = remove_apostrophe(data)\n",
        "    data = remove_stop_words(data)\n",
        "    data = convert_numbers(data)\n",
        "    data = stemming(data)\n",
        "    data = remove_punctuation(data)\n",
        "    data = convert_numbers(data)\n",
        "    data = stemming(data) \n",
        "    data = remove_punctuation(data) \n",
        "    data = remove_stop_words(data) \n",
        "    return data"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAxFE6JUHOJ9"
      },
      "source": [
        "processed_text = []\n",
        "\n",
        "for i in corpus[:N]:\n",
        "    text = i.strip()\n",
        "    processed_text.append(word_tokenize(str(preprocess(text))))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_svEwnO0HZ5M"
      },
      "source": [
        "### Creating Bag of Words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FDhDPGqHPtD"
      },
      "source": [
        "bowA = processed_text[0]\n",
        "bowB = processed_text[1]"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhtY4AzTHffb"
      },
      "source": [
        "wordSet = set(bowA).union(set(bowB))"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaIl79Y5Hgkw"
      },
      "source": [
        "wordDictA = dict.fromkeys(wordSet, 0)\n",
        "wordDictB = dict.fromkeys(wordSet, 0)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9Cs0uK4Hh8a",
        "outputId": "28e83184-ac4e-4fcb-acd0-d734c2b0ddf9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# check first 30 words\n",
        "import itertools\n",
        "dict(itertools.islice(wordDictA.items(), 30))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'age': 0,\n",
              " 'becam': 0,\n",
              " 'canada': 0,\n",
              " 'comfort': 0,\n",
              " 'confin': 0,\n",
              " 'earn': 0,\n",
              " 'equiti': 0,\n",
              " 'execut': 0,\n",
              " 'fame': 0,\n",
              " 'fund': 0,\n",
              " 'growth': 0,\n",
              " 'heavili': 0,\n",
              " 'howev': 0,\n",
              " 'internet': 0,\n",
              " 'late': 0,\n",
              " 'onlin': 0,\n",
              " 'pay': 0,\n",
              " 'peter': 0,\n",
              " 'prai': 0,\n",
              " 'public': 0,\n",
              " 'secur': 0,\n",
              " 'sedan': 0,\n",
              " 'sit': 0,\n",
              " 'take': 0,\n",
              " 'teen': 0,\n",
              " 'ten': 0,\n",
              " 'time': 0,\n",
              " 'twelv': 0,\n",
              " 'worth': 0,\n",
              " 'zip2': 0}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_KXC1x1HrBv"
      },
      "source": [
        "for word in bowA:\n",
        "    wordDictA[word] += 1\n",
        "\n",
        "    \n",
        "for word in bowB:\n",
        "    wordDictB[word] += 1"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ra2bOuyNIbMN",
        "outputId": "45a766d0-336c-4176-bef0-b7669b4264e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# re-check the change of first 30 words\n",
        "dict(itertools.islice(wordDictA.items(), 30))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'age': 2,\n",
              " 'becam': 1,\n",
              " 'canada': 0,\n",
              " 'comfort': 1,\n",
              " 'confin': 0,\n",
              " 'earn': 1,\n",
              " 'equiti': 0,\n",
              " 'execut': 1,\n",
              " 'fame': 1,\n",
              " 'fund': 1,\n",
              " 'growth': 0,\n",
              " 'heavili': 1,\n",
              " 'howev': 1,\n",
              " 'internet': 0,\n",
              " 'late': 1,\n",
              " 'onlin': 1,\n",
              " 'pay': 1,\n",
              " 'peter': 0,\n",
              " 'prai': 1,\n",
              " 'public': 1,\n",
              " 'secur': 1,\n",
              " 'sedan': 1,\n",
              " 'sit': 1,\n",
              " 'take': 1,\n",
              " 'teen': 0,\n",
              " 'ten': 1,\n",
              " 'time': 1,\n",
              " 'twelv': 2,\n",
              " 'worth': 0,\n",
              " 'zip2': 1}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gign-jrTIkpC",
        "outputId": "a0c7e057-f4e4-4490-d664-7eb8c038eaea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        }
      },
      "source": [
        "import pandas as pd\n",
        "CountVectorizer = pd.DataFrame([wordDictA, wordDictB])\n",
        "CountVectorizer"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>zip2</th>\n",
              "      <th>becam</th>\n",
              "      <th>fame</th>\n",
              "      <th>age</th>\n",
              "      <th>sedan</th>\n",
              "      <th>secur</th>\n",
              "      <th>growth</th>\n",
              "      <th>howev</th>\n",
              "      <th>peter</th>\n",
              "      <th>public</th>\n",
              "      <th>teen</th>\n",
              "      <th>comfort</th>\n",
              "      <th>fund</th>\n",
              "      <th>twelv</th>\n",
              "      <th>confin</th>\n",
              "      <th>equiti</th>\n",
              "      <th>execut</th>\n",
              "      <th>take</th>\n",
              "      <th>late</th>\n",
              "      <th>prai</th>\n",
              "      <th>earn</th>\n",
              "      <th>heavili</th>\n",
              "      <th>time</th>\n",
              "      <th>pay</th>\n",
              "      <th>canada</th>\n",
              "      <th>sit</th>\n",
              "      <th>onlin</th>\n",
              "      <th>internet</th>\n",
              "      <th>worth</th>\n",
              "      <th>ten</th>\n",
              "      <th>parent</th>\n",
              "      <th>left</th>\n",
              "      <th>histori</th>\n",
              "      <th>beaten</th>\n",
              "      <th>hundr</th>\n",
              "      <th>co</th>\n",
              "      <th>servic</th>\n",
              "      <th>list</th>\n",
              "      <th>continu</th>\n",
              "      <th>electr</th>\n",
              "      <th>...</th>\n",
              "      <th>defend</th>\n",
              "      <th>passport</th>\n",
              "      <th>degr</th>\n",
              "      <th>financ</th>\n",
              "      <th>call</th>\n",
              "      <th>firm</th>\n",
              "      <th>troubl</th>\n",
              "      <th>eighti</th>\n",
              "      <th>summer</th>\n",
              "      <th>launch</th>\n",
              "      <th>newspap</th>\n",
              "      <th>video</th>\n",
              "      <th>margin</th>\n",
              "      <th>doug</th>\n",
              "      <th>huge</th>\n",
              "      <th>eventu</th>\n",
              "      <th>largest</th>\n",
              "      <th>nineti</th>\n",
              "      <th>relaunch</th>\n",
              "      <th>octob</th>\n",
              "      <th>equip</th>\n",
              "      <th>eight</th>\n",
              "      <th>stage</th>\n",
              "      <th>queen</th>\n",
              "      <th>stanford</th>\n",
              "      <th>terribl</th>\n",
              "      <th>comput</th>\n",
              "      <th>well</th>\n",
              "      <th>transport</th>\n",
              "      <th>orbit</th>\n",
              "      <th>childhood</th>\n",
              "      <th>silicon</th>\n",
              "      <th>sinc</th>\n",
              "      <th>seven</th>\n",
              "      <th>econom</th>\n",
              "      <th>share</th>\n",
              "      <th>earli</th>\n",
              "      <th>spent</th>\n",
              "      <th>tweet</th>\n",
              "      <th>futur</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2 rows × 269 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   zip2  becam  fame  age  sedan  ...  share  earli  spent  tweet  futur\n",
              "0     1      1     1    2      1  ...      1      3      0      1      1\n",
              "1     3      1     0    1      0  ...      2      1      1      0      0\n",
              "\n",
              "[2 rows x 269 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMdRSTdQJD1x"
      },
      "source": [
        "# Term - Frequency"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fuicUu2gIupa"
      },
      "source": [
        "def Term_Frequency(wordDict, bow):\n",
        "    \"\"\"This function creates Term Frequency.\n",
        "\n",
        "    The Equation:\n",
        "    tf(t,d) = count of t in d / number of words in d.\"\"\"\n",
        "    tfDict = {}\n",
        "    bowCount = len(bow)\n",
        "    for word, count in wordDict.items():\n",
        "        tfDict[word] = count / float(bowCount)\n",
        "    return tfDict"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0vYTIMFIyrt"
      },
      "source": [
        "tfBowA = Term_Frequency(wordDictA, bowA)\n",
        "tfBowB = Term_Frequency(wordDictB, bowB)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5KJVXTcJL3o"
      },
      "source": [
        "# Inverse Document Frequency"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXzCm5hJI48-"
      },
      "source": [
        "def Inverse_doc_freq(docList):\n",
        "    \"\"\"This function creates Inverse-Document Freqeuncy\n",
        "    \n",
        "    The Equation:\n",
        "    idf(t) = log(N/(df + 1))\n",
        "    \"\"\"\n",
        "    import math\n",
        "    idfDict = {}\n",
        "    N = len(docList)\n",
        "    \n",
        "    # w kelimesini içeren dökümanları hesaplar\n",
        "    idfDict = dict.fromkeys(docList[0].keys(), 0)\n",
        "    \n",
        "    for doc in docList:\n",
        "        for word, val in doc.items():\n",
        "            if val > 0 :\n",
        "                idfDict[word] += 1\n",
        "    \n",
        "    # N sayısını paydada ki document-frequency'e bölüp logaritmasını alır.\n",
        "    for word, val in idfDict.items():\n",
        "        idfDict[word] = math.log(N / float(val+1))\n",
        "        return idfDict"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3bJafH8JVQn"
      },
      "source": [
        "idf = Inverse_doc_freq([wordDictA, wordDictB])"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79TQMWD0JYfe"
      },
      "source": [
        "# Term Frequency - Inverse Document Frequency (TF-IDF)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1jS_6xYJW2U"
      },
      "source": [
        "def TFIDF(tfBow, idf):\n",
        "    tfidf = {}\n",
        "    for word, val in tfBow.items():\n",
        "        if idf[word] < 0 :\n",
        "            idf[word] = idf[word] * (-1)\n",
        "        tfidf[word] = val * idf[word]\n",
        "    return tfidf"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFoC7EKLJgCM"
      },
      "source": [
        "tfidfA = TFIDF(tfBowA, idf)\n",
        "tfidfB = TFIDF(tfBowB, idf)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqxYj3_xJ4ni"
      },
      "source": [
        "# Result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwuX4xZFJgRw",
        "outputId": "619337ca-c223-4138-8fe1-c175c947f28d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        }
      },
      "source": [
        "pd.DataFrame([tfidfA, tfidfB])"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>zip2</th>\n",
              "      <th>becam</th>\n",
              "      <th>fame</th>\n",
              "      <th>age</th>\n",
              "      <th>sedan</th>\n",
              "      <th>secur</th>\n",
              "      <th>growth</th>\n",
              "      <th>howev</th>\n",
              "      <th>peter</th>\n",
              "      <th>public</th>\n",
              "      <th>teen</th>\n",
              "      <th>comfort</th>\n",
              "      <th>fund</th>\n",
              "      <th>twelv</th>\n",
              "      <th>confin</th>\n",
              "      <th>equiti</th>\n",
              "      <th>execut</th>\n",
              "      <th>take</th>\n",
              "      <th>late</th>\n",
              "      <th>prai</th>\n",
              "      <th>earn</th>\n",
              "      <th>heavili</th>\n",
              "      <th>time</th>\n",
              "      <th>pay</th>\n",
              "      <th>canada</th>\n",
              "      <th>sit</th>\n",
              "      <th>onlin</th>\n",
              "      <th>internet</th>\n",
              "      <th>worth</th>\n",
              "      <th>ten</th>\n",
              "      <th>parent</th>\n",
              "      <th>left</th>\n",
              "      <th>histori</th>\n",
              "      <th>beaten</th>\n",
              "      <th>hundr</th>\n",
              "      <th>co</th>\n",
              "      <th>servic</th>\n",
              "      <th>list</th>\n",
              "      <th>continu</th>\n",
              "      <th>electr</th>\n",
              "      <th>...</th>\n",
              "      <th>defend</th>\n",
              "      <th>passport</th>\n",
              "      <th>degr</th>\n",
              "      <th>financ</th>\n",
              "      <th>call</th>\n",
              "      <th>firm</th>\n",
              "      <th>troubl</th>\n",
              "      <th>eighti</th>\n",
              "      <th>summer</th>\n",
              "      <th>launch</th>\n",
              "      <th>newspap</th>\n",
              "      <th>video</th>\n",
              "      <th>margin</th>\n",
              "      <th>doug</th>\n",
              "      <th>huge</th>\n",
              "      <th>eventu</th>\n",
              "      <th>largest</th>\n",
              "      <th>nineti</th>\n",
              "      <th>relaunch</th>\n",
              "      <th>octob</th>\n",
              "      <th>equip</th>\n",
              "      <th>eight</th>\n",
              "      <th>stage</th>\n",
              "      <th>queen</th>\n",
              "      <th>stanford</th>\n",
              "      <th>terribl</th>\n",
              "      <th>comput</th>\n",
              "      <th>well</th>\n",
              "      <th>transport</th>\n",
              "      <th>orbit</th>\n",
              "      <th>childhood</th>\n",
              "      <th>silicon</th>\n",
              "      <th>sinc</th>\n",
              "      <th>seven</th>\n",
              "      <th>econom</th>\n",
              "      <th>share</th>\n",
              "      <th>earli</th>\n",
              "      <th>spent</th>\n",
              "      <th>tweet</th>\n",
              "      <th>futur</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.001423</td>\n",
              "      <td>0.007018</td>\n",
              "      <td>0.003509</td>\n",
              "      <td>0.014035</td>\n",
              "      <td>0.003509</td>\n",
              "      <td>0.003509</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.003509</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.003509</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.003509</td>\n",
              "      <td>0.003509</td>\n",
              "      <td>0.007018</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.003509</td>\n",
              "      <td>0.003509</td>\n",
              "      <td>0.003509</td>\n",
              "      <td>0.003509</td>\n",
              "      <td>0.003509</td>\n",
              "      <td>0.003509</td>\n",
              "      <td>0.003509</td>\n",
              "      <td>0.003509</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.003509</td>\n",
              "      <td>0.007018</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.003509</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.003509</td>\n",
              "      <td>0.003509</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.028070</td>\n",
              "      <td>0.003509</td>\n",
              "      <td>0.007018</td>\n",
              "      <td>0.003509</td>\n",
              "      <td>0.003509</td>\n",
              "      <td>0.010526</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.003509</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.017544</td>\n",
              "      <td>0.003509</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.003509</td>\n",
              "      <td>0.007018</td>\n",
              "      <td>0.003509</td>\n",
              "      <td>0.003509</td>\n",
              "      <td>0.003509</td>\n",
              "      <td>0.007018</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.014035</td>\n",
              "      <td>0.003509</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.003509</td>\n",
              "      <td>0.003509</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.007018</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.007018</td>\n",
              "      <td>0.007018</td>\n",
              "      <td>0.003509</td>\n",
              "      <td>0.003509</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.007018</td>\n",
              "      <td>0.007018</td>\n",
              "      <td>0.003509</td>\n",
              "      <td>0.007018</td>\n",
              "      <td>0.021053</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.003509</td>\n",
              "      <td>0.003509</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.005820</td>\n",
              "      <td>0.009569</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.009569</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.004785</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.004785</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.004785</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.004785</td>\n",
              "      <td>0.004785</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.004785</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.038278</td>\n",
              "      <td>0.004785</td>\n",
              "      <td>0.009569</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.004785</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.004785</td>\n",
              "      <td>0.057416</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.028708</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.004785</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.004785</td>\n",
              "      <td>0.004785</td>\n",
              "      <td>0.009569</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.004785</td>\n",
              "      <td>0.004785</td>\n",
              "      <td>0.004785</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.004785</td>\n",
              "      <td>0.019139</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.004785</td>\n",
              "      <td>0.004785</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.004785</td>\n",
              "      <td>0.009569</td>\n",
              "      <td>0.004785</td>\n",
              "      <td>0.019139</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.004785</td>\n",
              "      <td>0.004785</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.009569</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.019139</td>\n",
              "      <td>0.009569</td>\n",
              "      <td>0.004785</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2 rows × 269 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       zip2     becam      fame  ...     spent     tweet     futur\n",
              "0  0.001423  0.007018  0.003509  ...  0.000000  0.003509  0.003509\n",
              "1  0.005820  0.009569  0.000000  ...  0.004785  0.000000  0.000000\n",
              "\n",
              "[2 rows x 269 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    }
  ]
}